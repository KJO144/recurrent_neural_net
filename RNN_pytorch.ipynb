{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from simple_RNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input.txt'\n",
    "#filename = 'wodehouse_right_ho_jeeves.txt'\n",
    "data_raw = open(filename, 'r').read() # should be simple plain text file\n",
    "data_raw = data_raw.lower()\n",
    "\n",
    "data, vocab_size, idx_to_char = data_from_text(data_raw)\n",
    "data_one_hot = np.eye(vocab_size)[data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25 # this is how much of the data we sample before updating the params\n",
    "hidden_size = 50 # size of the hidden state vector\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, initial_params):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # create recurrent and linear layers\n",
    "        rnn = torch.nn.RNN(input_size=vocab_size,hidden_size=hidden_size)\n",
    "        linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "#        # set the weight tensors\n",
    "#         dtype = torch.float\n",
    "#         tensor_parameters = {name:torch.tensor(param, dtype=dtype) for name,param in parameters.items()}\n",
    "#         rnn.bias_ih_l0.data.fill_(0)\n",
    "#         rnn.weight_hh_l0.data = tensor_parameters['W']\n",
    "#         rnn.weight_ih_l0.data = tensor_parameters['U']\n",
    "#         rnn.bias_hh_l0.data = tensor_parameters['bh']\n",
    "#         linear.weight.data = tensor_parameters['V']\n",
    "#         linear.bias.data = tensor_parameters['by']\n",
    "        \n",
    "        self.rnn = rnn\n",
    "        self.linear = linear\n",
    "        self.h_prev = torch.zeros([1, 1, hidden_size])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        h, h_prev = self.rnn(inputs, self.h_prev)\n",
    "        out = self.linear(h)\n",
    "        self.h_prev = h_prev.detach()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(hidden_size, vocab_size)\n",
    "model = MyModel(vocab_size, hidden_size, parameters)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "#parameters = initialize_parameters(hidden_size, vocab_size)\n",
    "dtype = torch.float\n",
    "data_length = len(data)\n",
    "seqs_per_epoch = int(data_length/seq_length)  # will this work if data/seq divides exactly?\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-1)\n",
    "\n",
    "data3D = np.expand_dims(data_one_hot, 1)\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize hidden  state    \n",
    "    h_prev = torch.zeros([1, 1, hidden_size])\n",
    "    model.h_prev = h_prev\n",
    "    if verbose and epoch != 0:\n",
    "        print('epoch: {}, loss: {}'.format(epoch, loss))\n",
    "\n",
    "    for i in range(seqs_per_epoch):\n",
    "        start = i*seq_length\n",
    "        end = i*seq_length+seq_length\n",
    "        end = min(end, data_length-1)\n",
    "\n",
    "        inputs_raw = data3D[start:end]\n",
    "        targets_raw = data[start+1:end+1]\n",
    "             \n",
    "        # both inputs and targets are (seq_length, 1, vocab_size)\n",
    "        inputs = torch.tensor(inputs_raw, dtype=dtype)\n",
    "        targets = torch.tensor(targets_raw, dtype=torch.long)\n",
    "                \n",
    "        out = model(inputs)\n",
    "        \n",
    "        out2 = out.view((seq_length, vocab_size))\n",
    "        loss = loss_fn(out2, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print( i, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
