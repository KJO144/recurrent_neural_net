{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input.txt'\n",
    "#filename = 'wodehouse_right_ho_jeeves.txt'\n",
    "data_raw = open(filename, 'r').read() # should be simple plain text file\n",
    "data_raw = data_raw.lower()\n",
    "chars = list(set(data_raw))\n",
    "chars.sort()\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print('Data has length {} and consist of {} unique characters.'.format(len(data_raw), vocab_size))\n",
    "ch_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "data = [ch_to_idx[ch] for ch in data_raw]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25 # this is how much of the data we sample before updating the params\n",
    "hidden_size = 50 # size of the hidden state vector\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1\n",
    "m = len(data)//seq_length\n",
    "print( m, seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to get the data in the shape \n",
    "data_one_hot = tf.keras.utils.to_categorical(data)\n",
    "data_X = data_one_hot[0:m*seq_length]\n",
    "data_Y = data_one_hot[1:m*seq_length+1]\n",
    "\n",
    "data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (seq_length, vocab_size)\n",
    "X_ph = tf.placeholder(tf.float32, input_shape)\n",
    "Y_ph = tf.placeholder(tf.float32, input_shape)\n",
    "h_prev_ph = tf.placeholder(tf.float32, (hidden_size,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_RNN import initialize_parameters\n",
    "params = initialize_parameters(hidden_size, vocab_size)\n",
    "\n",
    "param_tensors = {name: tf.Variable(param, dtype=tf.float32) for name,param in params.items()}\n",
    "\n",
    "U = param_tensors['U']\n",
    "V = param_tensors['V']\n",
    "W = param_tensors['W']\n",
    "by = param_tensors['by']\n",
    "bh = param_tensors['bh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "loss_tensor = 0\n",
    "\n",
    "h = h_prev_ph\n",
    "\n",
    "h_series = []\n",
    "\n",
    "for t in range(seq_length):\n",
    "    xt = X_ph[t,:]  # vocab_size\n",
    "    yt = Y_ph[t,:]  # vocab_size\n",
    "    xt = tf.reshape(xt, [vocab_size,1] ) # vocab_size x 1\n",
    "    yt = tf.reshape(yt, [vocab_size,1] ) # vocab_size x 1\n",
    "    \n",
    "    p = tf.matmul(U, xt ) + tf.matmul(W, h) + bh # hidden_size x 1\n",
    "    h = tf.tanh(p) # hidden_size x 1\n",
    "    h_series.append(h)\n",
    "\n",
    "labels_series = tf.unstack(Y_ph, axis=0)\n",
    "outputs = [tf.matmul(V,hh) + by for hh in h_series]\n",
    "losses = [tf.nn.softmax_cross_entropy_with_logits_v2(logits=out, labels=tf.reshape(yt, (vocab_size,1)),axis=0) for out,yt in zip(outputs,labels_series)]\n",
    "loss_tensor = tf.reduce_sum(losses)\n",
    "\n",
    "# keep the gradients as well (don't need this, but it's nice to compare to other implementations)\n",
    "param_names = params.keys()\n",
    "tensors = [param_tensors[pn] for pn in param_names]\n",
    "grads_list = tf.gradients(loss_tensor, tensors)\n",
    "grads_dict = dict(zip(param_names, grads_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_tensor)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate/10).minimize(loss_tensor)\n",
    "seqs_per_epoch = int(len(data)/seq_length)\n",
    "\n",
    "num_epochs = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        h_prev = np.zeros((hidden_size,1), dtype=float)\n",
    "        for i in range(seqs_per_epoch):\n",
    "            start = i*seq_length\n",
    "            end = i*seq_length+seq_length\n",
    "\n",
    "            X = data_X[start:end]\n",
    "            Y = data_Y[start:end]\n",
    "            \n",
    "            _, loss, grads, h_prev = sess.run([optimizer, loss_tensor, grads_dict, h], feed_dict={X_ph: X, Y_ph: Y, h_prev_ph: h_prev})\n",
    "        print('epoch: {}, iter: {}, loss: {}'.format(epoch, i, loss))\n",
    "\n",
    "    trained_params = sess.run(param_tensors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_RNN import generate_sample\n",
    "trained_params\n",
    "h_prev = np.zeros((hidden_size,1), dtype=float)\n",
    "\n",
    "sample_indices = generate_sample(trained_params, h_prev, 0, 100)\n",
    "sample = ''.join([idx_to_char[idx] for idx in sample_indices])\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
